{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PandisDP/Deep-Reinforcement-Learning/blob/main/DRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jI1k7MRPGy3U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import count\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple\n",
        "from IPython import display\n",
        "import torch\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9j5nWlb66RM"
      },
      "outputs": [],
      "source": [
        "class Field:\n",
        "    def __init__(self,device,size,item_pickup,item_dropoff,\n",
        "                start_position,zones_blocks=[],path_predicts='Episodes'):\n",
        "        '''\n",
        "        Constructor of the class Field\n",
        "        Parameters:\n",
        "        size: size of the field\n",
        "        item_pickup: position of the item to pickup\n",
        "        item_dropoff: position of the item to dropoff\n",
        "        start_position: position of the agent\n",
        "        zones_blocks: list of tuples with the positions of the blocks\n",
        "        path_predicts: path to save the images of the episodes\n",
        "        '''\n",
        "        self.device=device\n",
        "        self.size = size\n",
        "        self.item_pickup = item_pickup\n",
        "        self.item_dropoff = item_dropoff\n",
        "        self.position = start_position\n",
        "        self.position_start= start_position\n",
        "        self.block_zones=zones_blocks\n",
        "        self.item_in_car= False\n",
        "        self.number_of_actions=6\n",
        "        self.allposicions = []\n",
        "        self.path_predicts= path_predicts\n",
        "        self.done=False\n",
        "        self.save_state()\n",
        "        self.initial_state = {\n",
        "            'device': self.device,\n",
        "            'position': self.position,\n",
        "            'item_pickup': self.item_pickup,\n",
        "            'item_dropoff': self.item_dropoff,\n",
        "            'item_in_car': self.item_in_car\n",
        "        }\n",
        "\n",
        "    def reset(self):\n",
        "        '''\n",
        "        Reset the game\n",
        "        '''\n",
        "        self.device = self.initial_state['device']\n",
        "        self.position = self.initial_state['position']\n",
        "        self.item_pickup = self.initial_state['item_pickup']\n",
        "        self.item_dropoff = self.initial_state['item_dropoff']\n",
        "        self.item_in_car = self.initial_state['item_in_car']\n",
        "        self.done=False\n",
        "        self.allposicions = []\n",
        "        self.save_state()    \n",
        "\n",
        "    def get_number_of_actions(self):\n",
        "        '''\n",
        "        Get the number of actions of the game\n",
        "        Returns: number of actions\n",
        "        '''\n",
        "        return self.number_of_actions\n",
        "    \n",
        "    def get_number_of_states(self):\n",
        "        '''\n",
        "        Get the number of states of the game\n",
        "        Returns: number of states\n",
        "        '''\n",
        "        return (self.size**4)*2 \n",
        "\n",
        "    def get_state(self):\n",
        "        '''\n",
        "        Get the state of the game\n",
        "        Returns: state\n",
        "        '''\n",
        "        state= self.position[0]*self.size*self.size*self.size*2\n",
        "        state+= self.position[1]*self.size*self.size*2\n",
        "        state+= self.item_pickup[0]*self.size*2\n",
        "        state+= self.item_pickup[1]*2   \n",
        "        if self.item_in_car:\n",
        "            state+=1\n",
        "        return torch.tensor([state],device=self.device)   \n",
        "    \n",
        "    def save_state(self):\n",
        "        '''\n",
        "        Save the state of the game\n",
        "        '''\n",
        "        self.allposicions.append(self.position)\n",
        "\n",
        "    def graphics(self,puntos,name_fig):\n",
        "        '''\n",
        "        Create a plot of the game\n",
        "        Parameters:\n",
        "        puntos: list of tuples with the positions of the points\n",
        "        name_fig: name of the figure\n",
        "        '''\n",
        "        # Crear una cuadrícula de 10x10\n",
        "        cuadricula = np.zeros((10, 10))\n",
        "        # Marcar los puntos en la cuadrícula\n",
        "        for punto in puntos:\n",
        "            cuadricula[punto] = 1\n",
        "        # Crear la figura y el eje para el plot\n",
        "        fig, ax = plt.subplots()\n",
        "        # Usar 'imshow' para mostrar la cuadrícula como una imagen\n",
        "        # 'cmap' define el mapa de colores, 'Greys' es bueno para gráficos en blanco y negro\n",
        "        ax.imshow(cuadricula, cmap='Greys', origin='lower')\n",
        "        # Ajustar los ticks para que coincidan con las posiciones de la cuadrícula\n",
        "        ax.set_xticks(np.arange(-.5, 10, 1))\n",
        "        ax.set_yticks(np.arange(-.5, 10, 1))\n",
        "        # Dibujar las líneas de la cuadrícula\n",
        "        ax.grid(color='black', linestyle='-', linewidth=2)\n",
        "        # Ajustar el límite para evitar cortes\n",
        "        ax.set_xlim(-0.5, 9.5)\n",
        "        ax.set_ylim(-0.5, 9.5)\n",
        "        for punto in self.block_zones:\n",
        "            ax.scatter(punto[1], punto[0], color='red', marker='X', s=100) \n",
        "        for punto in puntos:\n",
        "            ax.text(punto[1], punto[0], '✔', color='white', ha='center', va='center', fontsize=10)\n",
        "\n",
        "        lst_start=[self.position_start, self.item_pickup,self.item_dropoff]\n",
        "        for punto in lst_start:\n",
        "            ax.scatter(punto[1], punto[0], color='blue',marker='*', s=100)  \n",
        "        name_fig_path = self.path_predicts + '/' +name_fig\n",
        "        plt.savefig(name_fig_path)\n",
        "        plt.close()\n",
        "\n",
        "    def empty_predict_data(self):\n",
        "        '''\n",
        "        Empty the folder of the predictions\n",
        "        '''\n",
        "        path=self.path_predicts\n",
        "        for nombre in os.listdir(path):\n",
        "            ruta_completa = os.path.join(path, nombre)\n",
        "            try:\n",
        "                if os.path.isfile(ruta_completa) or os.path.islink(ruta_completa):\n",
        "                    os.remove(ruta_completa)\n",
        "                elif os.path.isdir(ruta_completa):\n",
        "                    shutil.rmtree(ruta_completa)\n",
        "            except Exception as e:\n",
        "                print(f'Error {ruta_completa}. reason: {e}')\n",
        "\n",
        "    def block_zones_evaluation(self,position):\n",
        "        '''\n",
        "        Evaluate if the position is in a block zone\n",
        "        Parameters:\n",
        "        position: position to evaluate\n",
        "        Returns: True if the position is in a block zone, False otherwise\n",
        "        '''\n",
        "        if position in self.block_zones:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def make_action(self,action):\n",
        "        '''\n",
        "        Make an action in the game\n",
        "        Parameters:\n",
        "        action: action to make\n",
        "        Returns: reward, done\n",
        "        '''\n",
        "        val_return=0\n",
        "        (x,y) = self.position\n",
        "        if action ==0: #down\n",
        "            if y==self.size-1:\n",
        "                val_return= -10 #reward,done\n",
        "                return torch.tensor([val_return],device=self.device),self.done\n",
        "            else:\n",
        "                self.position = (x,y+1)\n",
        "                self.save_state()\n",
        "                if self.block_zones_evaluation(self.position):\n",
        "                    val_return= -100\n",
        "                    return torch.tensor([val_return],device=self.device),self.done \n",
        "                val_return = -1\n",
        "                return torch.tensor([val_return],device=self.device),self.done\n",
        "        elif action ==1: #up\n",
        "            if y==0:\n",
        "                val_return = -10\n",
        "                return torch.tensor([val_return],device=self.device),self.done  \n",
        "            else:\n",
        "                self.position = (x,y-1)\n",
        "                self.save_state()\n",
        "                if self.block_zones_evaluation(self.position):\n",
        "                    val_return =-100\n",
        "                    return torch.tensor([val_return],device=self.device),self.done  \n",
        "                val_return = -1\n",
        "                return torch.tensor([val_return],device=self.device),self.done\n",
        "        elif action ==2: #left\n",
        "            if x==0:\n",
        "                val_return = -10\n",
        "                return torch.tensor([val_return],device=self.device),self.done  \n",
        "            else:\n",
        "                self.position = (x-1,y)\n",
        "                self.save_state()\n",
        "                if self.block_zones_evaluation(self.position):\n",
        "                    val_return = -100\n",
        "                    return torch.tensor([val_return],device=self.device),self.done  \n",
        "                val_return= -1\n",
        "                return torch.tensor([val_return],device=self.device),self.done  \n",
        "        elif action ==3: #right\n",
        "            if x==self.size-1:\n",
        "                val_return = -10\n",
        "                return torch.tensor([val_return],device=self.device),self.done  \n",
        "            else:\n",
        "                self.position = (x+1,y)\n",
        "                self.save_state()\n",
        "                if self.block_zones_evaluation(self.position):\n",
        "                    val_return =-100\n",
        "                    return torch.tensor([val_return],device=self.device),self.done  \n",
        "                val_return = -1\n",
        "                return torch.tensor([val_return],device=self.device),self.done \n",
        "        elif action ==4: #pickup\n",
        "            if self.item_in_car:\n",
        "                val_return = -10\n",
        "                return torch.tensor([val_return],device=self.device),self.done   \n",
        "            elif self.item_pickup != (x,y):\n",
        "                val_return = -10\n",
        "                return torch.tensor([val_return],device=self.device),self.done  \n",
        "            else:\n",
        "                self.item_in_car = True\n",
        "                val_return = 20\n",
        "                return torch.tensor([val_return],device=self.device),self.done\n",
        "        elif action ==5: #dropoff\n",
        "            if not self.item_in_car:\n",
        "                val_return = -10\n",
        "                return torch.tensor([val_return],device=self.device),self.done  \n",
        "            elif self.item_dropoff != (x,y):\n",
        "                val_return = -10\n",
        "                return torch.tensor([val_return],device=self.device),self.done   \n",
        "            else:\n",
        "                self.item_in_car = False\n",
        "                self.done=True\n",
        "                val_return = 20\n",
        "                return torch.tensor([val_return],device=self.device),self.done  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryboZsJ8D4_m"
      },
      "outputs": [],
      "source": [
        "class QValues():\n",
        "    device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #device = th.device(\"mps\" if th.backends.mps.is_available() else \"cpu\")\n",
        "    @staticmethod\n",
        "    def get_current(policy_net,states,actions):\n",
        "        value_=policy_net(states).gather(dim=1,index=actions.unsqueeze(-1))\n",
        "        return value_\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_next(target_net,next_states,is_done):\n",
        "        next_q_values= torch.zeros(len(next_states)).to(QValues.device)\n",
        "        non_final_mask= ~is_done\n",
        "        non_final_next_states= next_states[non_final_mask]\n",
        "        if len(non_final_next_states)>0:\n",
        "            with torch.no_grad():\n",
        "                next_q_values[non_final_mask]= target_net(non_final_next_states).max(dim=1)[0]\n",
        "        return next_q_values\n",
        "        \n",
        "class DQN(nn.Module):\n",
        "    def __init__(self,feature_size, num_actions):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features=feature_size,out_features=128)\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=256)\n",
        "        self.fc3 = nn.Linear(in_features=256, out_features=128)\n",
        "        self.out= nn.Linear(in_features=128 ,out_features=num_actions)\n",
        "\n",
        "    def forward(self,t): \n",
        "    \n",
        "        if t.dim()==1:\n",
        "            t= t.unsqueeze(1)\n",
        "        t=t.float()  \n",
        "        t= F.relu(self.fc1(t))\n",
        "        t= F.relu(self.fc2(t))\n",
        "        t= F.relu(self.fc3(t))\n",
        "        t= self.out(t)\n",
        "        return t    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FoG1FXGEtBe"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Agent():\n",
        "    def __init__(self,strategy,num_actions,device):\n",
        "        self.step=0\n",
        "        self.strategy=strategy\n",
        "        self.num_actions= num_actions\n",
        "        self.device=device\n",
        "\n",
        "    def select_action(self,state,policy_net):\n",
        "        rate= self.strategy.get_exploration_rate(self.step)\n",
        "        self.step+=1\n",
        "        if random.random()<rate:\n",
        "            action= random.randrange(self.num_actions)\n",
        "            return torch.tensor([action]).to(self.device) #action\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                return policy_net(state).argmax(dim=1).to(self.device)\n",
        "\n",
        "class EpsilonGreedyStrategy():\n",
        "    def __init__(self,start,end,decay):\n",
        "        self.start= start\n",
        "        self.end= end\n",
        "        self.decay= decay\n",
        "\n",
        "    def get_exploration_rate(self,step):\n",
        "        return self.end + (self.start - self.end)*math.exp(-step*self.decay)    \n",
        "\n",
        "class ReplayMemory():\n",
        "    def __init__(self,capacity):\n",
        "        self.capacity= capacity\n",
        "        self.memory= []\n",
        "        self.count=0\n",
        "    def push(self,exp):\n",
        "        if len(self.memory)< self.capacity:\n",
        "            self.memory.append(exp)\n",
        "        else:\n",
        "            self.memory[self.count%self.capacity]=exp\n",
        "        self.count+=1      \n",
        "    def sample(self,batch_size):\n",
        "        return random.sample(self.memory,batch_size),0,0\n",
        "\n",
        "    def can_provide_sample(self,batch_size):\n",
        "        return len(self.memory)> batch_size \n",
        "\n",
        "class PrioritizedReplayMemory():\n",
        "    def __init__(self, capacity, alpha=0.6):\n",
        "        self.tree = SumTree(capacity)\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.beta = 0.4\n",
        "        self.beta_increment_per_sampling = 0.001\n",
        "        self.epsilon = 1e-6\n",
        "\n",
        "    def _get_priority(self, error):\n",
        "        return (error + self.epsilon) ** self.alpha\n",
        "\n",
        "    def push(self, error, sample):\n",
        "        p = self._get_priority(error)\n",
        "        self.tree.add(p, sample)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = []\n",
        "        idxs = []\n",
        "        segment = self.tree.total() / batch_size\n",
        "        priorities = []\n",
        "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
        "        for i in range(batch_size):\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "            s = random.uniform(a, b)\n",
        "            s = min(max(s, 0), self.tree.total())\n",
        "            (idx, p, data) = self.tree.get(s)\n",
        "            #print('bath',idx,p,self.tree.total(),data,a,b,s)\n",
        "            batch.append(data)\n",
        "            idxs.append(idx)\n",
        "            priorities.append(p)\n",
        "\n",
        "        sampling_probabilities = priorities / self.tree.total()\n",
        "        sampling_probabilities+=self.epsilon\n",
        "        is_weights = np.power(self.tree.total() * sampling_probabilities, -self.beta)\n",
        "        is_weights /= is_weights.max()\n",
        "        #print('bath',idx,p,data)\n",
        "        return batch, idxs, is_weights\n",
        "\n",
        "    def update(self, idx, error):\n",
        "        p = self._get_priority(error)\n",
        "        self.tree.update(idx, p)\n",
        "\n",
        "    def can_provide_sample(self, batch_size):\n",
        "        return self.tree.write >= batch_size  \n",
        "\n",
        "\n",
        "class SumTree:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        self.data = np.zeros(capacity, dtype=object)\n",
        "        self.write = 0\n",
        "\n",
        "    def _propagate(self, idx, change):\n",
        "        parent = (idx - 1) // 2\n",
        "        self.tree[parent] += change\n",
        "\n",
        "        if parent != 0:\n",
        "            self._propagate(parent, change)\n",
        "\n",
        "    def _retrieve(self, idx, s):\n",
        "        left = 2 * idx + 1\n",
        "        right = left + 1\n",
        "\n",
        "        if left >= len(self.tree):\n",
        "            return idx\n",
        "\n",
        "        if s <= self.tree[left]:\n",
        "            return self._retrieve(left, s)\n",
        "        else:\n",
        "            if self.tree[right] > 0:\n",
        "                return self._retrieve(right, s - self.tree[left])\n",
        "            else:\n",
        "                return idx\n",
        "\n",
        "    def total(self):\n",
        "        return self.tree[0]\n",
        "\n",
        "    def add(self, p, data):\n",
        "        idx = self.write + self.capacity - 1\n",
        "        self.data[self.write] = data\n",
        "        self.update(idx, p)\n",
        "        self.write += 1\n",
        "        if self.write >= self.capacity:\n",
        "            self.write = 0\n",
        "\n",
        "    def update(self, idx, p):\n",
        "        change = p - self.tree[idx]\n",
        "        self.tree[idx] = p\n",
        "        self._propagate(idx, change)\n",
        "\n",
        "    def get(self, s):\n",
        "        idx = self._retrieve(0, s)\n",
        "        dataIdx = idx - self.capacity + 1\n",
        "\n",
        "        return idx, self.tree[idx], self.data[dataIdx]                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2lvLyBJHeuc",
        "outputId": "59b1516b-2776-4a65-8bdd-90232609b488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Episode:  0  Average_Losses:  135.28777499408346  Duration:  389215\n",
            "Episode:  1  Average_Losses:  4.105564534663412  Duration:  122408\n",
            "Episode:  2  Average_Losses:  172.5556785041706  Duration:  221875\n",
            "Episode:  3  Average_Losses:  127.1155606902137  Duration:  257766\n",
            "Episode:  4  Average_Losses:  289.5835316890957  Duration:  18484\n",
            "Episode:  5  Average_Losses:  265.1592773839858  Duration:  16709\n",
            "Episode:  6  Average_Losses:  219.13382037182205  Duration:  98785\n",
            "Episode:  7  Average_Losses:  229.6904303624651  Duration:  6165\n",
            "Episode:  8  Average_Losses:  157.07802212904772  Duration:  24039\n",
            "Episode:  9  Average_Losses:  195.22051784399622  Duration:  86371\n",
            "Episode:  10  Average_Losses:  218.35436321994868  Duration:  14417\n",
            "Episode:  11  Average_Losses:  152.2117819436007  Duration:  58571\n",
            "Episode:  12  Average_Losses:  181.87751747064297  Duration:  215796\n",
            "Episode:  13  Average_Losses:  163.18956641947008  Duration:  48555\n",
            "Episode:  14  Average_Losses:  175.40653724211737  Duration:  8898\n",
            "Episode:  15  Average_Losses:  191.46316395471558  Duration:  25415\n",
            "Episode:  16  Average_Losses:  186.3344355247012  Duration:  8030\n",
            "Episode:  17  Average_Losses:  215.62522621852597  Duration:  3853\n",
            "Episode:  18  Average_Losses:  176.41984365503663  Duration:  39792\n",
            "Episode:  19  Average_Losses:  168.7968355624024  Duration:  71044\n",
            "Episode:  20  Average_Losses:  181.1456279356248  Duration:  15870\n",
            "Episode:  21  Average_Losses:  253.0537759127529  Duration:  2387\n",
            "Episode:  22  Average_Losses:  264.82683419783245  Duration:  2395\n",
            "Episode:  23  Average_Losses:  181.70505139277256  Duration:  41392\n",
            "Episode:  24  Average_Losses:  147.92961221247745  Duration:  38807\n",
            "Episode:  25  Average_Losses:  162.06002199518397  Duration:  69485\n",
            "Episode:  26  Average_Losses:  144.79269601459427  Duration:  10802\n",
            "Episode:  27  Average_Losses:  185.519657398762  Duration:  11490\n",
            "Episode:  28  Average_Losses:  150.22344305087316  Duration:  173821\n",
            "Episode:  29  Average_Losses:  130.2568569334661  Duration:  9264\n",
            "Episode:  30  Average_Losses:  163.92723763834329  Duration:  73757\n",
            "Episode:  31  Average_Losses:  177.0229715465804  Duration:  51950\n",
            "Episode:  32  Average_Losses:  216.78156852903228  Duration:  3686\n",
            "Episode:  33  Average_Losses:  205.166249729058  Duration:  7861\n",
            "Episode:  34  Average_Losses:  156.98586369478733  Duration:  83269\n",
            "Episode:  35  Average_Losses:  131.06797396707546  Duration:  53528\n",
            "Episode:  36  Average_Losses:  140.10669015933877  Duration:  217113\n",
            "Episode:  37  Average_Losses:  120.58760578142521  Duration:  63104\n",
            "Episode:  38  Average_Losses:  163.75338370905297  Duration:  30753\n",
            "Episode:  39  Average_Losses:  143.6942059600542  Duration:  100047\n"
          ]
        }
      ],
      "source": [
        "from QL import DQN,QValues\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import count\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple\n",
        "from IPython import display\n",
        "\n",
        "Experience= namedtuple('Experience',('state','action','next_state','reward','is_done'))\n",
        "\n",
        "class QDQ:\n",
        "    def __init__(self,device,enviroment,agent,memory_strategy,features):\n",
        "        self.device= device\n",
        "        self.env= enviroment\n",
        "        self.memory= memory_strategy\n",
        "        self.features= features\n",
        "        self.agent= agent\n",
        "        self.policy_net= DQN(self.features,self.env.get_number_of_actions()).to(self.device)\n",
        "        self.target_net= DQN(self.features,self.env.get_number_of_actions()).to(self.device)\n",
        "\n",
        "    def training_priorized_memory(self,batch_size,gamma,target_update,learning_rate,num_episodes):\n",
        "        print('Training Process with Prioritized Memory')\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict()) \n",
        "        self.target_net.eval()\n",
        "        optimizer= th.optim.Adam(self.policy_net.parameters(),lr=learning_rate)\n",
        "        episode_durations=[]\n",
        "        episode_losses=[]\n",
        "        total_timesteps = 0\n",
        "        for episode in range(num_episodes):\n",
        "            self.env.reset()\n",
        "            episode_losses=[]\n",
        "            total_loss= 0\n",
        "            loss_count= 0\n",
        "            for timestep in count():\n",
        "                state= self.env.get_state()\n",
        "                action= self.agent.select_action(state,self.policy_net)\n",
        "                reward,done= self.env.make_action(action)\n",
        "                next_state= self.env.get_state()\n",
        "                # Calculate the error for the priority\n",
        "                with th.no_grad():\n",
        "                    current_q_value = self.policy_net(state).gather(1, th.tensor([[action]]))\n",
        "                    next_q_value = self.target_net(next_state).max(1)[0].unsqueeze(1)\n",
        "                    target_q_value = reward + (gamma * next_q_value * (1 - done))\n",
        "                    error = abs(current_q_value - target_q_value).item()\n",
        "                self.memory.push(error, Experience(state, action, next_state, reward, done))\n",
        "                #memory.push(Experience(state,action,next_state,reward,done))\n",
        "                if self.memory.can_provide_sample(batch_size):\n",
        "                    experiences,idxs,is_weights= self.memory.sample(batch_size)\n",
        "                    states,actions,rewards,next_states,is_done= self.__extract_tensors(experiences)\n",
        "                    current_q_values= QValues.get_current(self.policy_net,states,actions)\n",
        "                    with th.no_grad():\n",
        "                        next_q_values= QValues.get_next(self.target_net,next_states,is_done)\n",
        "                    target_q_values= (next_q_values*gamma)+rewards\n",
        "                    is_weights= th.tensor(is_weights,dtype=th.float).unsqueeze(1)\n",
        "                    #loss= F.mse_loss(current_q_values,target_q_values.unsqueeze(1))\n",
        "                    loss = (is_weights * F.mse_loss(current_q_values, target_q_values.unsqueeze(1)\n",
        "                                                    , reduction='none')).mean()\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    # Update priorities\n",
        "                    errors = th.abs(current_q_values - target_q_values.unsqueeze(1)).detach().numpy()\n",
        "                    for idx, error in zip(idxs, errors):\n",
        "                        self.memory.update(idx, error)\n",
        "                    episode_losses.append(loss.item())\n",
        "                    total_timesteps += 1\n",
        "                if done:\n",
        "                    episode_durations.append(timestep)\n",
        "                    print(\"Episode: \",episode,\" Average_Losses: \",np.mean(episode_losses),\n",
        "                        \" Duration: \",timestep)   \n",
        "                    break\n",
        "                if total_timesteps % target_update == 0:\n",
        "                    self.target_net.load_state_dict(self.policy_net.state_dict())     \n",
        "\n",
        "    def training_replay_memory(self,batch_size,gamma,target_update,learning_rate,num_episodes):\n",
        "        print('Training Process with Replay Memory')\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "        optimizer= th.optim.Adam(self.policy_net.parameters(),lr=learning_rate)\n",
        "        episode_durations=[]\n",
        "        episode_losses=[]\n",
        "        total_timesteps = 0\n",
        "        for episode in range(num_episodes):\n",
        "            self.env.reset()\n",
        "            episode_losses=[]\n",
        "            total_loss= 0\n",
        "            loss_count= 0\n",
        "            for timestep in count():\n",
        "                state= self.env.get_state()\n",
        "                action= self.agent.select_action(state,self.policy_net)\n",
        "                reward,done= self.env.make_action(action)\n",
        "                next_state= self.env.get_state()\n",
        "                self.memory.push(Experience(state,action,next_state,reward,done))\n",
        "                if self.memory.can_provide_sample(batch_size):\n",
        "                    experiences,*_= self.memory.sample(batch_size)\n",
        "                    #print(experiences)\n",
        "                    states,actions,rewards,next_states,is_done= self.__extract_tensors(experiences)\n",
        "                    current_q_values= QValues.get_current(self.policy_net,states,actions)\n",
        "                    with th.no_grad():\n",
        "                        next_q_values= QValues.get_next(self.target_net,next_states,is_done)\n",
        "                    target_q_values= (next_q_values*gamma)+rewards\n",
        "                    loss= F.mse_loss(current_q_values,target_q_values.unsqueeze(1))\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    episode_losses.append(loss.item())\n",
        "                    total_timesteps += 1\n",
        "                if done:\n",
        "                    episode_durations.append(timestep)\n",
        "                    print(\"Episode: \",episode,\" Average_Losses: \",np.mean(episode_losses),\n",
        "                        \" Duration: \",timestep)\n",
        "                    break\n",
        "                if total_timesteps % target_update == 0:\n",
        "                    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "\n",
        "    def __extract_tensors(self,experiences):\n",
        "        batch = Experience(*zip(*experiences))\n",
        "        states = th.cat(batch.state)\n",
        "        actions = th.cat(batch.action)\n",
        "        rewards = th.cat(batch.reward)\n",
        "        next_states = th.cat(batch.next_state)\n",
        "        final_states = th.tensor(batch.is_done, dtype=th.bool)\n",
        "        return states, actions, rewards, next_states, final_states\n",
        "\n",
        "    def __get_moving_avg(self,values,period):\n",
        "        values = th.tensor(values,dtype=th.float)\n",
        "        if len(values)>=period:\n",
        "            moving_avg= values.unfold(dimension=0,size=period,step=1).mean(dim=1).flatten(start_dim=0)\n",
        "            moving_avg= th.cat((th.zeros(period-1),moving_avg))\n",
        "            return moving_avg\n",
        "        else:\n",
        "            moving_avg= th.zeros(len(values))\n",
        "            return moving_avg\n",
        "        \n",
        "    def __plot(self,values,moving_avg_period):\n",
        "        plt.figure(2)\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Duration')\n",
        "        plt.plot(values)\n",
        "        moving_avg= self.get_moving_avg(values,moving_avg_period)\n",
        "        plt.plot(moving_avg)\n",
        "        plt.pause(0.001)\n",
        "        #print(\"Episode\", len(values),\"\\n\",moving_avg_period,\"episode moving avg:\", moving_avg[-1])\n",
        "        display.clear_output(wait=True)            \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from QDQ  import QDQ\n",
        "from  Games import Field\n",
        "from Agent import Agent ,EpsilonGreedyStrategy,ReplayMemory,PrioritizedReplayMemory\n",
        "import torch as th\n",
        "\n",
        "def training_process(params_env,prms_tra,type_memory):\n",
        "    '''\n",
        "    Params:\n",
        "    params_env: Dictionary with the parameters of the game\n",
        "    prms_tra: Dictionary with the parameters of the training\n",
        "    type_memory: 0 for ReplayMemory and 1 for PrioritizedReplayMemory\n",
        "    '''\n",
        "    device= th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
        "    env= Field(device,params_env['size'],params_env['start_position'],params_env['item_pickup'],\n",
        "                params_env['item_dropoff'],params_env['zones_block'],params_env['Path'])\n",
        "    eps= EpsilonGreedyStrategy(prms_tra['eps_start'],prms_tra['eps_end'],prms_tra['eps_decay'])\n",
        "    agent= Agent(eps,env.get_number_of_actions(),device)\n",
        "    if type_memory==0:\n",
        "        memory= ReplayMemory(prms_tra['memory_size'])\n",
        "        q= QDQ(device,env,agent,memory,prms_tra['features'])\n",
        "        q.training_replay_memory(prms_tra['batch_size'],prms_tra['gamma'],\n",
        "                            prms_tra['target_update'],prms_tra['lr'],prms_tra['num_episodes'])\n",
        "    elif type_memory==1:\n",
        "        memory= PrioritizedReplayMemory(prms_tra['memory_size'])\n",
        "        q= QDQ(device,env,agent,memory,prms_tra['features'])\n",
        "        q.training_priorized_memory(prms_tra['batch_size'],prms_tra['gamma'],\n",
        "                            prms_tra['target_update'],prms_tra['lr'],prms_tra['num_episodes'])\n",
        "    else:\n",
        "        print('Error: type_memory must be 0 or 1')\n",
        "        return 'Error' \n",
        "    return 0\n",
        "if __name__ == '__main__':\n",
        "    params_game = {\n",
        "                \"size\": 10,\n",
        "                \"start_position\": (9, 0),  # (9,0)\n",
        "                \"item_pickup\": (1, 1),  # (1,1)\n",
        "                \"item_dropoff\": (8, 8),  # (8,8)\n",
        "                \"zones_block\": [(4, 0), (4, 1), (4, 2), (4, 3), (2, 6), (2, 7), (2, 8), (2, 9), \n",
        "                                (4, 8), (5, 8), (6, 8), (7, 6), (8, 6), (9, 6)],\n",
        "                \"Path\": 'Episodes'\n",
        "    }\n",
        "    params_training = {\n",
        "                \"batch_size\": 128,\n",
        "                \"features\": 1,\n",
        "                \"gamma\": 0.99,\n",
        "                \"eps_start\": 1,\n",
        "                \"eps_end\": 0.01,\n",
        "                \"eps_decay\": 0.001,\n",
        "                \"target_update\": 5000,\n",
        "                \"memory_size\": 100000,\n",
        "                \"lr\": 0.001,\n",
        "                \"num_episodes\": 10000\n",
        "    }\n",
        "    training_process(params_game,params_training,0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyORFVHJ/+QQBjAcOY4nd3jT",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

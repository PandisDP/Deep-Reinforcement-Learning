{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PandisDP/Deep-Reinforcement-Learning/blob/main/DRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jI1k7MRPGy3U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import count\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple\n",
        "from IPython import display\n",
        "import torch\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Class Field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Field:\n",
        "    def __init__(self,device,size,item_pickup,item_dropoff,\n",
        "                start_position,zones_blocks=[],path_predicts='Episodes'):\n",
        "        '''\n",
        "        Constructor of the class Field\n",
        "        Parameters:\n",
        "        device: device to run the game\n",
        "        size: size of the field\n",
        "        item_pickup: position of the item to pickup\n",
        "        item_dropoff: position of the item to dropoff\n",
        "        start_position: position of the agent\n",
        "        zones_blocks: list of tuples with the positions of the blocks\n",
        "        path_predicts: path to save the images of the episodes\n",
        "        '''\n",
        "        self.device=device\n",
        "        self.size = size\n",
        "        self.item_pickup = item_pickup\n",
        "        self.item_dropoff = item_dropoff\n",
        "        self.position = start_position\n",
        "        self.position_start= start_position\n",
        "        self.block_zones=zones_blocks\n",
        "        self.item_in_car= False\n",
        "        self.number_of_actions=6\n",
        "        self.allposicions = []\n",
        "        self.path_predicts= path_predicts\n",
        "        self.done=False\n",
        "        self.save_state()\n",
        "        self.initial_state = {\n",
        "            'device': self.device,\n",
        "            'position': self.position,\n",
        "            'item_pickup': self.item_pickup,\n",
        "            'item_dropoff': self.item_dropoff,\n",
        "            'item_in_car': self.item_in_car\n",
        "        }\n",
        "\n",
        "    def reset(self):\n",
        "        '''\n",
        "        Reset the game\n",
        "        '''\n",
        "        self.device = self.initial_state['device']\n",
        "        self.position = self.initial_state['position']\n",
        "        self.item_pickup = self.initial_state['item_pickup']\n",
        "        self.item_dropoff = self.initial_state['item_dropoff']\n",
        "        self.item_in_car = self.initial_state['item_in_car']\n",
        "        self.done=False\n",
        "        self.allposicions = []\n",
        "        self.save_state()    \n",
        "\n",
        "    def get_number_of_actions(self):\n",
        "        '''\n",
        "        Get the number of actions of the game\n",
        "        Returns: number of actions\n",
        "        '''\n",
        "        return self.number_of_actions\n",
        "    \n",
        "    def get_number_of_states(self):\n",
        "        '''\n",
        "        Get the number of states of the game\n",
        "        Returns: number of states\n",
        "        '''\n",
        "        return (self.size**4)*2 \n",
        "\n",
        "    def get_state(self):\n",
        "        '''\n",
        "        Get the state of the game\n",
        "        Returns: state\n",
        "        '''\n",
        "        state= self.position[0]*self.size*self.size*self.size*2\n",
        "        state+= self.position[1]*self.size*self.size*2\n",
        "        state+= self.item_pickup[0]*self.size*2\n",
        "        state+= self.item_pickup[1]*2   \n",
        "        if self.item_in_car:\n",
        "            state+=1\n",
        "        return torch.tensor([state],device=self.device)   \n",
        "    \n",
        "    def save_state(self):\n",
        "        '''\n",
        "        Save the state of the game\n",
        "        '''\n",
        "        self.allposicions.append(self.position)\n",
        "\n",
        "    def graphics(self,puntos,name_fig):\n",
        "        '''\n",
        "        Create a plot of the game\n",
        "        Parameters:\n",
        "        puntos: list of tuples with the positions of the points\n",
        "        name_fig: name of the figure\n",
        "        '''\n",
        "        # Crear una cuadrícula de 10x10\n",
        "        cuadricula = np.zeros((10, 10))\n",
        "        # Marcar los puntos en la cuadrícula\n",
        "        for punto in puntos:\n",
        "            cuadricula[punto] = 1\n",
        "        # Crear la figura y el eje para el plot\n",
        "        fig, ax = plt.subplots()\n",
        "        # Usar 'imshow' para mostrar la cuadrícula como una imagen\n",
        "        # 'cmap' define el mapa de colores, 'Greys' es bueno para gráficos en blanco y negro\n",
        "        ax.imshow(cuadricula, cmap='Greys', origin='lower')\n",
        "        # Ajustar los ticks para que coincidan con las posiciones de la cuadrícula\n",
        "        ax.set_xticks(np.arange(-.5, 10, 1))\n",
        "        ax.set_yticks(np.arange(-.5, 10, 1))\n",
        "        # Dibujar las líneas de la cuadrícula\n",
        "        ax.grid(color='black', linestyle='-', linewidth=2)\n",
        "        # Ajustar el límite para evitar cortes\n",
        "        ax.set_xlim(-0.5, 9.5)\n",
        "        ax.set_ylim(-0.5, 9.5)\n",
        "        for punto in self.block_zones:\n",
        "            ax.scatter(punto[1], punto[0], color='red', marker='X', s=100) \n",
        "        for punto in puntos:\n",
        "            ax.text(punto[1], punto[0], '✔', color='white', ha='center', va='center', fontsize=10)\n",
        "\n",
        "        lst_start=[self.position_start, self.item_pickup,self.item_dropoff]\n",
        "        for punto in lst_start:\n",
        "            ax.scatter(punto[1], punto[0], color='blue',marker='*', s=100)  \n",
        "        name_fig_path = self.path_predicts + '/' +name_fig\n",
        "        plt.savefig(name_fig_path)\n",
        "        plt.close()\n",
        "\n",
        "    def empty_predict_data(self):\n",
        "        '''\n",
        "        Empty the folder of the predictions\n",
        "        '''\n",
        "        path=self.path_predicts\n",
        "        for nombre in os.listdir(path):\n",
        "            ruta_completa = os.path.join(path, nombre)\n",
        "            try:\n",
        "                if os.path.isfile(ruta_completa) or os.path.islink(ruta_completa):\n",
        "                    os.remove(ruta_completa)\n",
        "                elif os.path.isdir(ruta_completa):\n",
        "                    shutil.rmtree(ruta_completa)\n",
        "            except Exception as e:\n",
        "                print(f'Error {ruta_completa}. reason: {e}')\n",
        "\n",
        "    def block_zones_evaluation(self,position):\n",
        "        '''\n",
        "        Evaluate if the position is in a block zone\n",
        "        Parameters:\n",
        "        position: position to evaluate\n",
        "        Returns: True if the position is in a block zone, False otherwise\n",
        "        '''\n",
        "        if position in self.block_zones:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def make_action(self,action):\n",
        "        '''\n",
        "        Make an action in the game\n",
        "        Parameters:\n",
        "        action: action to make\n",
        "        Returns: reward, done\n",
        "        '''\n",
        "        val_return=0\n",
        "        (x,y) = self.position\n",
        "        if action ==0: #down\n",
        "            if y==self.size-1:\n",
        "                val_return= -10 #reward,done\n",
        "                return torch.tensor([val_return],device=self.device),self.done\n",
        "            else:\n",
        "                self.position = (x,y+1)\n",
        "                self.save_state()\n",
        "                if self.block_zones_evaluation(self.position):\n",
        "                    val_return= -100\n",
        "                    return torch.tensor([val_return],device=self.device),self.done \n",
        "                val_return = -1\n",
        "                return torch.tensor([val_return],device=self.device),self.done\n",
        "        elif action ==1: #up\n",
        "            if y==0:\n",
        "                val_return = -10\n",
        "                return torch.tensor([val_return],device=self.device),self.done  \n",
        "            else:\n",
        "                self.position = (x,y-1)\n",
        "                self.save_state()\n",
        "                if self.block_zones_evaluation(self.position):\n",
        "                    val_return =-100\n",
        "                    return torch.tensor([val_return],device=self.device),self.done  \n",
        "                val_return = -1\n",
        "                return torch.tensor([val_return],device=self.device),self.done\n",
        "        elif action ==2: #left\n",
        "            if x==0:\n",
        "                val_return = -10\n",
        "                return torch.tensor([val_return],device=self.device),self.done  \n",
        "            else:\n",
        "                self.position = (x-1,y)\n",
        "                self.save_state()\n",
        "                if self.block_zones_evaluation(self.position):\n",
        "                    val_return = -100\n",
        "                    return torch.tensor([val_return],device=self.device),self.done  \n",
        "                val_return= -1\n",
        "                return torch.tensor([val_return],device=self.device),self.done  \n",
        "        elif action ==3: #right\n",
        "            if x==self.size-1:\n",
        "                val_return = -10\n",
        "                return torch.tensor([val_return],device=self.device),self.done  \n",
        "            else:\n",
        "                self.position = (x+1,y)\n",
        "                self.save_state()\n",
        "                if self.block_zones_evaluation(self.position):\n",
        "                    val_return =-100\n",
        "                    return torch.tensor([val_return],device=self.device),self.done  \n",
        "                val_return = -1\n",
        "                return torch.tensor([val_return],device=self.device),self.done \n",
        "        elif action ==4: #pickup\n",
        "            if self.item_in_car:\n",
        "                val_return = -10\n",
        "                return torch.tensor([val_return],device=self.device),self.done   \n",
        "            elif self.item_pickup != (x,y):\n",
        "                val_return = -10\n",
        "                return torch.tensor([val_return],device=self.device),self.done  \n",
        "            else:\n",
        "                self.item_in_car = True\n",
        "                val_return = 20\n",
        "                return torch.tensor([val_return],device=self.device),self.done\n",
        "        elif action ==5: #dropoff\n",
        "            if not self.item_in_car:\n",
        "                val_return = -10\n",
        "                return torch.tensor([val_return],device=self.device),self.done  \n",
        "            elif self.item_dropoff != (x,y):\n",
        "                val_return = -10\n",
        "                return torch.tensor([val_return],device=self.device),self.done   \n",
        "            else:\n",
        "                self.item_in_car = False\n",
        "                self.done=True\n",
        "                val_return = 20\n",
        "                return torch.tensor([val_return],device=self.device),self.done  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Class QValues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryboZsJ8D4_m"
      },
      "outputs": [],
      "source": [
        "\n",
        "class QValues():\n",
        "    '''\n",
        "    This class is used to manage the Q-values of the agent\n",
        "    '''\n",
        "    def __init__(self,device):\n",
        "        '''\n",
        "        Params:\n",
        "        device: The device used to run the agent'''\n",
        "        self.device= device\n",
        "    def get_current(self,policy_net,states,actions):\n",
        "        '''\n",
        "        This method is used to get the Q-values of the current state\n",
        "        Params:\n",
        "        policy_net: The neural network used to calculate the Q-values\n",
        "        states: The states of the agent\n",
        "        actions: The actions of the agent\n",
        "        Returns: The Q-values of the current state'''\n",
        "        return policy_net(states).gather(dim=1,index=actions.unsqueeze(-1))\n",
        "    \n",
        "    def get_current_i(self,policy_net,state,action):\n",
        "        '''\n",
        "        This method is used to get the Q-values of the current state\n",
        "        Params:\n",
        "        policy_net: The neural network used to calculate the Q-values\n",
        "        state: The state of the agent\n",
        "        action: The action of the agent\n",
        "        Returns: The Q-values of the current state'''\n",
        "        return policy_net(state).gather(1, th.tensor([[action]], device=self.device))\n",
        "    \n",
        "    def get_next_i(self, target_net, next_state):\n",
        "        '''\n",
        "        This method is used to get the Q-values of the next state\n",
        "        Params:\n",
        "        target_net: The target neural network used to calculate the Q-values\n",
        "        next_state: The next state of the agent\n",
        "        Returns: The Q-values of the next state'''\n",
        "        return target_net(next_state).max(1)[0].unsqueeze(1)\n",
        "    \n",
        "    def get_next(self,target_net,next_states,is_done):\n",
        "        '''\n",
        "        This method is used to get the Q-values of the next state\n",
        "        Params:\n",
        "        target_net: The target neural network used to calculate the Q-values\n",
        "        next_states: The next states of the agent\n",
        "        is_done: A boolean array that indicates if the episode is done\n",
        "        Returns: The Q-values of the next state'''\n",
        "        next_q_values= torch.zeros(len(next_states)).to(self.device)\n",
        "        non_final_mask= ~is_done\n",
        "        non_final_next_states= next_states[non_final_mask]\n",
        "        if len(non_final_next_states)>0:\n",
        "            next_q_values[non_final_mask]= target_net(non_final_next_states).max(dim=1)[0]\n",
        "        return next_q_values\n",
        "        \n",
        "class DQN(nn.Module):\n",
        "    '''\n",
        "    This class is used to manage the neural network of the agent\n",
        "    This arquiteture is used in the Deep Q-Learning algorithm but is possible\n",
        "    change for diferentes problems or games.\n",
        "    '''\n",
        "    def __init__(self,feature_size, num_actions):\n",
        "        '''\n",
        "        Params:\n",
        "        feature_size: The size of the features\n",
        "        num_actions: The number of actions that the agent can take'''\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features=feature_size,out_features=128)\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=256)\n",
        "        self.fc3 = nn.Linear(in_features=256, out_features=128)\n",
        "        self.out= nn.Linear(in_features=128 ,out_features=num_actions)\n",
        "\n",
        "    def forward(self,t): \n",
        "        '''\n",
        "        This method calculates the Q-values of the agent.\n",
        "        \n",
        "        Params:\n",
        "        t (Tensor): The input features of the agent.\n",
        "        Returns:\n",
        "        Tensor: The Q-values of the agent.\n",
        "        '''\n",
        "    \n",
        "        if t.dim()==1:\n",
        "            t= t.unsqueeze(1)\n",
        "        t=t.float()  \n",
        "        t= F.relu(self.fc1(t))\n",
        "        t= F.relu(self.fc2(t))\n",
        "        t= F.relu(self.fc3(t))\n",
        "        t= self.out(t)\n",
        "        return t    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Class Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FoG1FXGEtBe"
      },
      "outputs": [],
      "source": [
        "\n",
        "#This class is used to manage the agent\n",
        "class Agent():\n",
        "    '''\n",
        "    This class is used to manage the agent\n",
        "    '''\n",
        "    def __init__(self,strategy,num_actions,device):\n",
        "        '''\n",
        "        Params:\n",
        "        strategy: The strategy used to select the actions\n",
        "        num_actions: The number of actions that the agent can take\n",
        "        device: The device used to run the agent'''\n",
        "        self.step=0\n",
        "        self.strategy=strategy\n",
        "        self.num_actions= num_actions\n",
        "        self.device=device\n",
        "\n",
        "    def select_action(self,state,policy_net):\n",
        "        '''\n",
        "        This method is used to select an action for the agent\n",
        "        throught exploration or exploitation\n",
        "        Params:\n",
        "        state: The state of the agent\n",
        "        policy_net: The neural network used to calculate the Q-values\n",
        "        Returns: The action selected by the agent'''\n",
        "        rate= self.strategy.get_exploration_rate(self.step)\n",
        "        self.step+=1\n",
        "        if random.random()<rate:\n",
        "            action= random.randrange(self.num_actions)\n",
        "            return torch.tensor([action]).to(self.device) #action\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                return policy_net(state).argmax(dim=1).to(self.device)\n",
        "\n",
        "class EpsilonGreedyStrategy():\n",
        "    '''\n",
        "    This class is used to manage the exploration rate of the agent\n",
        "    '''\n",
        "    def __init__(self,start,end,decay):\n",
        "        '''\n",
        "        Params:\n",
        "        start: The initial exploration rate\n",
        "        end: The final exploration rate\n",
        "        decay: The decay of the exploration rate'''\n",
        "        self.start= start\n",
        "        self.end= end\n",
        "        self.decay= decay\n",
        "\n",
        "    def get_exploration_rate(self,step):\n",
        "        '''\n",
        "        This method is used to get the exploration rate of the agent\n",
        "        Params:\n",
        "        step: The step of the training process\n",
        "        Returns: The exploration rate of the agent'''\n",
        "        return self.end + (self.start - self.end)*math.exp(-step*self.decay)    \n",
        "\n",
        "class ReplayMemory():\n",
        "    '''\n",
        "    This class is used to store the experiences of the agent\n",
        "    '''\n",
        "    def __init__(self,capacity):\n",
        "        '''\n",
        "        Params:\n",
        "        capacity: The maximum number of experiences that the memory can store\n",
        "        memory: List with the experiences\n",
        "        count: The number of experiences stored in the memory'''\n",
        "        self.capacity= capacity\n",
        "        self.memory= []\n",
        "        self.count=0\n",
        "    def push(self,exp):\n",
        "        '''\n",
        "        This method is used to store an experience in the memory\n",
        "        Params:\n",
        "        exp: The experience to store in the memory\n",
        "        '''\n",
        "        if len(self.memory)< self.capacity:\n",
        "            self.memory.append(exp)\n",
        "        else:\n",
        "            self.memory[self.count%self.capacity]=exp\n",
        "        self.count+=1      \n",
        "    def sample(self,batch_size):\n",
        "        '''\n",
        "        This method is used to get a sample of experiences from the memory\n",
        "        Params:\n",
        "        batch_size: The number of experiences to get from the memory\n",
        "        Returns: A sample of experiences from the memory\n",
        "        '''\n",
        "        return random.sample(self.memory,batch_size),0,0\n",
        "\n",
        "    def can_provide_sample(self,batch_size):\n",
        "        '''\n",
        "        This method is used to check if the memory has enough experiences to provide a sample\n",
        "        Params:\n",
        "        batch_size: The number of experiences to get from the memory\n",
        "        Returns: True if the memory has enough experiences to provide a sample, False otherwise\n",
        "        '''\n",
        "        return len(self.memory)> batch_size \n",
        "\n",
        "class PrioritizedReplayMemory():\n",
        "    '''\n",
        "    This class is used to store the experiences of the agent with priorities\n",
        "    '''\n",
        "    def __init__(self, capacity, alpha=0.6):\n",
        "        '''\n",
        "        Params:\n",
        "        capacity: The maximum number of experiences that the memory can store\n",
        "        alpha: The exponent used to calculate the priority of the experiences\n",
        "        beta: The exponent used to calculate the importance sampling weights\n",
        "        beta_increment_per_sampling: The increment of beta for each sampling\n",
        "        epsilon: A small value to avoid division by zero\n",
        "        '''\n",
        "        self.tree = SumTree(capacity)\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.beta = 0.4\n",
        "        self.beta_increment_per_sampling = 0.001\n",
        "        self.epsilon = 1e-6\n",
        "\n",
        "    def _get_priority(self, error):\n",
        "        '''\n",
        "        This method is used to calculate the priority of an experience\n",
        "        Params:\n",
        "        error: The error of the experience\n",
        "        Returns: The priority of the experience'''\n",
        "        return (error + self.epsilon) ** self.alpha\n",
        "\n",
        "    def push(self, error, sample):\n",
        "        '''\n",
        "        This method is used to store an experience in the memory\n",
        "        Params:\n",
        "        error: The error of the experience\n",
        "        sample: The experience to store in the memory\n",
        "        '''\n",
        "        p = self._get_priority(error)\n",
        "        self.tree.add(p, sample)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        '''\n",
        "        This method is used to get a sample of experiences from the memory\n",
        "        Params:\n",
        "        batch_size: The number of experiences to get from the memory\n",
        "        Returns: A sample of experiences from the memory'''\n",
        "        batch = []\n",
        "        idxs = []\n",
        "        segment = self.tree.total() / batch_size\n",
        "        priorities = []\n",
        "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
        "        for i in range(batch_size):\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "            s = random.uniform(a, b)\n",
        "            s = min(max(s, 0), self.tree.total())\n",
        "            (idx, p, data) = self.tree.get(s)\n",
        "            batch.append(data)\n",
        "            idxs.append(idx)\n",
        "            priorities.append(p)\n",
        "\n",
        "        sampling_probabilities = priorities / self.tree.total()\n",
        "        sampling_probabilities+=self.epsilon\n",
        "        is_weights = np.power(self.tree.total() * sampling_probabilities, -self.beta)\n",
        "        is_weights /= is_weights.max()\n",
        "        return batch, idxs, is_weights\n",
        "\n",
        "    def update(self, idx, error):\n",
        "        '''\n",
        "        This method is used to update the priority of an experience\n",
        "        Params:\n",
        "        idx: The index of the experience\n",
        "        error: The error of the experience\n",
        "        '''\n",
        "        p = self._get_priority(error)\n",
        "        self.tree.update(idx, p)\n",
        "\n",
        "    def can_provide_sample(self, batch_size):\n",
        "        '''\n",
        "        This method is used to check if the memory has enough experiences to provide a sample\n",
        "        Params:\n",
        "        batch_size: The number of experiences to get from the memory\n",
        "        Returns: True if the memory has enough experiences to provide a sample, False otherwise'''\n",
        "        return self.tree.write >= batch_size  \n",
        "\n",
        "\n",
        "class SumTree:\n",
        "    '''\n",
        "    This class is used to store the priorities of the experiences'''\n",
        "    def __init__(self, capacity):\n",
        "        '''\n",
        "        Params:\n",
        "        capacity: The maximum number of experiences that the memory can store\n",
        "        tree: The binary tree used to store the priorities\n",
        "        data: The experiences stored in the memory\n",
        "        write: The number of experiences stored in the memory\n",
        "        '''\n",
        "        self.capacity = capacity\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        self.data = np.zeros(capacity, dtype=object)\n",
        "        self.write = 0\n",
        "        self.visited_nodes = set()\n",
        "\n",
        "    def _propagate(self, idx, change):\n",
        "        '''\n",
        "        This method is used to update the priorities of the experiences\n",
        "        Params:\n",
        "        idx: The index of the experience\n",
        "        change: The change in the priority of the experience'''\n",
        "        parent = (idx - 1) // 2\n",
        "        self.tree[parent] += change\n",
        "        self.visited_nodes.add(parent)\n",
        "        if parent > 0:\n",
        "            self._propagate(parent, change)\n",
        "\n",
        "    def _retrieve(self, idx, s):\n",
        "        '''\n",
        "        This method is used to get the index of an experience\n",
        "        Params:\n",
        "        idx: The index of the experience\n",
        "        s: The priority of the experience\n",
        "        Returns: The index of the experience'''\n",
        "        left = 2 * idx + 1\n",
        "        right = left + 1\n",
        "        if left >= len(self.tree):\n",
        "            return idx\n",
        "        if s <= self.tree[left] and left in self.visited_nodes:\n",
        "            return self._retrieve(left, s)\n",
        "        else:\n",
        "            if right in self.visited_nodes:\n",
        "                return self._retrieve(right, s - self.tree[left])\n",
        "            else:\n",
        "                return idx\n",
        "\n",
        "    def total(self):\n",
        "        '''\n",
        "        This method is used to get the total priority of the experiences\n",
        "        Returns: The total priority of the experiences'''\n",
        "        return self.tree[0]\n",
        "\n",
        "    def add(self, p, data):\n",
        "        '''\n",
        "        This method is used to store an experience in the memory\n",
        "        Params:\n",
        "        p: The priority of the experience\n",
        "        data: The experience to store in the memory\n",
        "        '''\n",
        "        idx = self.write + self.capacity - 1\n",
        "        self.data[self.write] = data\n",
        "        self.update(idx, p)\n",
        "        self.write += 1\n",
        "        if self.write >= self.capacity:\n",
        "            self.write = 0\n",
        "\n",
        "    def update(self, idx, p):\n",
        "        '''\n",
        "        This method is used to update the priority of an experience and \n",
        "        propagate the change to the parent nodes\n",
        "        Params:\n",
        "        idx: The index of the experience\n",
        "        p: The priority of the experience\n",
        "        '''\n",
        "        change = p - self.tree[idx]\n",
        "        self.tree[idx] = p\n",
        "        self.visited_nodes.add(idx)\n",
        "        self._propagate(idx, change)\n",
        "\n",
        "    def get(self, s):\n",
        "        '''\n",
        "        This method is used to get an experience from the memory\n",
        "        Params:\n",
        "        s: The priority of the experience\n",
        "        Returns: The index, priority, and experience of the experience\n",
        "        '''\n",
        "        idx = self._retrieve(0, s)\n",
        "        dataIdx = idx - self.capacity + 1\n",
        "        return idx, self.tree[idx], self.data[dataIdx]                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Class QDQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2lvLyBJHeuc",
        "outputId": "59b1516b-2776-4a65-8bdd-90232609b488"
      },
      "outputs": [],
      "source": [
        "Experience= namedtuple('Experience',('state','action','next_state','reward','is_done'))\n",
        "\n",
        "class QDQ:\n",
        "    '''\n",
        "    This class is used to manage the training process of the agent\n",
        "    '''\n",
        "    def __init__(self,device,enviroment,agent,memory_strategy,features):\n",
        "        '''\n",
        "        Params:\n",
        "        device: The device used to run the agent\n",
        "        enviroment: The enviroment of the game\n",
        "        agent: The agent of the game\n",
        "        memory_strategy: The memory strategy used to store the experiences\n",
        "        features: The number of dimensiones of the game'''\n",
        "        self.device= device\n",
        "        self.env= enviroment\n",
        "        self.memory= memory_strategy\n",
        "        self.features= features\n",
        "        self.agent= agent\n",
        "        self.policy_net= DQN(self.features,self.env.get_number_of_actions()).to(self.device)\n",
        "        self.target_net= DQN(self.features,self.env.get_number_of_actions()).to(self.device)\n",
        "        self.qvalue= QValues(device)\n",
        "\n",
        "    def training_priorized_memory(self,batch_size,gamma,target_update,\n",
        "                                learning_rate,num_episodes,checkpoint_file='checkpoint.pth'):\n",
        "        '''\n",
        "        This method is used to train the agent using the Prioritized Memory\n",
        "        Params:\n",
        "        batch_size: The number of experiences to get from the memory\n",
        "        gamma: The discount factor\n",
        "        target_update: The number of episodes to update the target network\n",
        "        learning_rate: The learning rate of the training process\n",
        "        num_episodes: The number of episodes to train the agent\n",
        "        checkpoint_file: The name of the file to save the checkpoint\n",
        "        '''\n",
        "        print('Training Process with Prioritized Memory')\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict()) \n",
        "        self.target_net.eval()\n",
        "        optimizer= th.optim.Adam(self.policy_net.parameters(),lr=learning_rate)\n",
        "        episode_durations=[]\n",
        "        episode_losses=[]\n",
        "        total_timesteps = 0\n",
        "        # Load checkpoint if exists and get the start episode\n",
        "        start_episode=0\n",
        "        try:\n",
        "            start_episode= self.load_checkpoint(optimizer,checkpoint_file)\n",
        "        except FileNotFoundError:\n",
        "            print('No checkpoint found starting from scratch')    \n",
        "        for episode in range(start_episode,num_episodes):\n",
        "            self.env.reset()\n",
        "            for timestep in count():\n",
        "                state= self.env.get_state()\n",
        "                action= self.agent.select_action(state,self.policy_net)\n",
        "                reward,done= self.env.make_action(action)\n",
        "                next_state= self.env.get_state()\n",
        "                done = th.tensor([done], device=self.device, dtype=th.bool)\n",
        "                with th.no_grad():\n",
        "                    current_q_value = self.qvalue.get_current_i(self.policy_net, state, action)\n",
        "                    next_q_value = self.qvalue.get_next_i(self.target_net, next_state)\n",
        "                    target_q_value = reward + (gamma * next_q_value * (1 - done.float()))\n",
        "                    error = abs(current_q_value - target_q_value).item()    \n",
        "                self.memory.push(error, Experience(state, action, next_state, reward, done))\n",
        "                if self.memory.can_provide_sample(batch_size):\n",
        "                    experiences,idxs,is_weights= self.memory.sample(batch_size)\n",
        "                    states,actions,rewards,next_states,is_done= self.__extract_tensors(experiences)\n",
        "                    current_q_values= self.qvalue.get_current(self.policy_net,states,actions)\n",
        "                    with th.no_grad():\n",
        "                        next_q_values= self.qvalue.get_next(self.target_net,next_states,is_done)\n",
        "                    target_q_values= (next_q_values*gamma)+rewards\n",
        "                    is_weights= th.tensor(is_weights,dtype=th.float).unsqueeze(1).to(self.device)\n",
        "                    loss = (is_weights * F.mse_loss(current_q_values, target_q_values.unsqueeze(1)\n",
        "                                                    , reduction='none')).mean()\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_timesteps += 1\n",
        "                    errors = th.abs(current_q_values - target_q_values.unsqueeze(1)).detach()\n",
        "                    for idx, error in zip(idxs, errors):\n",
        "                        self.memory.update(idx, error.item())    \n",
        "                if done:\n",
        "                    episode_durations.append(timestep)\n",
        "                    if 'loss' in locals():\n",
        "                        avg_loss = loss.item()\n",
        "                    else:\n",
        "                        avg_loss = 0   \n",
        "                    print(\"Episode: \",episode,\" Average_Losses: \",avg_loss,\n",
        "                        \" Duration: \",timestep)  \n",
        "                    self.save_checkpoint(episode,optimizer) \n",
        "                    break\n",
        "                if total_timesteps % target_update == 0:\n",
        "                    self.target_net.load_state_dict(self.policy_net.state_dict())     \n",
        "\n",
        "    def training_replay_memory(self,batch_size,gamma,target_update,learning_rate,\n",
        "                            num_episodes,checkpoint_file='checkpoint.pth'):\n",
        "        '''\n",
        "        This method is used to train the agent using the Replay Memory\n",
        "        Params:\n",
        "        batch_size: The number of experiences to get from the memory\n",
        "        gamma: The discount factor\n",
        "        target_update: The number of episodes to update the target network\n",
        "        learning_rate: The learning rate of the training process\n",
        "        num_episodes: The number of episodes to train the agent\n",
        "        checkpoint_file: The name of the file to save the checkpoint\n",
        "        '''\n",
        "        print('Training Process with Replay Memory')\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "        optimizer= th.optim.Adam(self.policy_net.parameters(),lr=learning_rate)\n",
        "        episode_durations=[]\n",
        "        episode_losses=[]\n",
        "        total_timesteps = 0\n",
        "        # Load checkpoint if exists and get the start episode\n",
        "        start_episode=0\n",
        "        try:\n",
        "            start_episode= self.load_checkpoint(optimizer,checkpoint_file)\n",
        "        except FileNotFoundError:\n",
        "            print('No checkpoint found starting from scratch')  \n",
        "        for episode in range(start_episode,num_episodes):\n",
        "            self.env.reset()\n",
        "            for timestep in count():\n",
        "                state= self.env.get_state()\n",
        "                action= self.agent.select_action(state,self.policy_net)\n",
        "                reward,done= self.env.make_action(action)\n",
        "                next_state= self.env.get_state()\n",
        "                done = th.tensor([done], device=self.device, dtype=th.bool)\n",
        "                self.memory.push(Experience(state,action,next_state,reward,done))\n",
        "                if self.memory.can_provide_sample(batch_size):\n",
        "                    experiences,*_= self.memory.sample(batch_size)\n",
        "                    states,actions,rewards,next_states,is_done= self.__extract_tensors(experiences)\n",
        "                    current_q_values= self.qvalue.get_current(self.policy_net,states,actions)\n",
        "                    with th.no_grad():\n",
        "                        next_q_values= self.qvalue.get_next(self.target_net,next_states,is_done)\n",
        "                    target_q_values= (next_q_values*gamma)+rewards\n",
        "                    loss= F.mse_loss(current_q_values,target_q_values.unsqueeze(1))\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_timesteps += 1\n",
        "                if done:\n",
        "                    episode_durations.append(timestep)\n",
        "                    if 'loss' in locals():\n",
        "                        avg_loss = loss.item()\n",
        "                    else:\n",
        "                        avg_loss = 0   \n",
        "                    print(\"Episode: \",episode,\" Average_Losses: \",avg_loss,\n",
        "                        \" Duration: \",timestep) \n",
        "                    self.save_checkpoint(episode,optimizer)\n",
        "                    break\n",
        "                if total_timesteps % target_update == 0:\n",
        "                    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "    def save_checkpoint(self, episode, optimizer, filename='checkpoint.pth'):\n",
        "        checkpoint = {\n",
        "            'episode': episode,\n",
        "            'model_state_dict': self.policy_net.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'target_net_state_dict': self.target_net.state_dict()\n",
        "        }\n",
        "        th.save(checkpoint, filename)\n",
        "        print(f'Checkpoint saved at episode {episode}')\n",
        "\n",
        "    def load_checkpoint(self, optimizer, filename='checkpoint.pth'):\n",
        "        if th.cuda.is_available():\n",
        "            checkpoint = th.load(filename)\n",
        "        else:\n",
        "            checkpoint = th.load(filename, map_location=th.device('cpu'))\n",
        "        \n",
        "        self.policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_episode = checkpoint['episode']\n",
        "        print(f'Checkpoint loaded from episode {start_episode}')\n",
        "        return start_episode    \n",
        "\n",
        "    def __extract_tensors(self,experiences):\n",
        "        batch = Experience(*zip(*experiences))\n",
        "        states = th.cat(batch.state).to(self.device)\n",
        "        actions = th.cat(batch.action).to(self.device)\n",
        "        rewards = th.cat(batch.reward).to(self.device)\n",
        "        next_states = th.cat(batch.next_state).to(self.device)\n",
        "        final_states = th.tensor(batch.is_done, dtype=th.bool).to(self.device)\n",
        "        return states, actions, rewards, next_states, final_states\n",
        "\n",
        "    def __get_moving_avg(self,values,period):\n",
        "        values = th.tensor(values,dtype=th.float)\n",
        "        if len(values)>=period:\n",
        "            moving_avg= values.unfold(dimension=0,size=period,step=1).mean(dim=1).flatten(start_dim=0)\n",
        "            moving_avg= th.cat((th.zeros(period-1),moving_avg))\n",
        "            return moving_avg\n",
        "        else:\n",
        "            moving_avg= th.zeros(len(values))\n",
        "            return moving_avg\n",
        "        \n",
        "    def __plot(self,values,moving_avg_period):\n",
        "        plt.figure(2)\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Duration')\n",
        "        plt.plot(values)\n",
        "        moving_avg= self.get_moving_avg(values,moving_avg_period)\n",
        "        plt.plot(moving_avg)\n",
        "        plt.pause(0.001)\n",
        "        #print(\"Episode\", len(values),\"\\n\",moving_avg_period,\"episode moving avg:\", moving_avg[-1])\n",
        "        display.clear_output(wait=True)            \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def training_process(params_env,prms_tra,type_memory):\n",
        "    '''\n",
        "    Params:\n",
        "    params_env: Dictionary with the parameters of the game\n",
        "    prms_tra: Dictionary with the parameters of the training\n",
        "    type_memory: 0 for ReplayMemory and 1 for PrioritizedReplayMemory\n",
        "    '''\n",
        "    #device= th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
        "    device = th.device(\"mps\" if th.backends.mps.is_available() else \"cpu\")\n",
        "    print('The training process used this Device: ',device)\n",
        "    env= Field(device,params_env['size'],params_env['start_position'],params_env['item_pickup'],\n",
        "                params_env['item_dropoff'],params_env['zones_block'],params_env['Path'])\n",
        "    eps= EpsilonGreedyStrategy(prms_tra['eps_start'],prms_tra['eps_end'],prms_tra['eps_decay'])\n",
        "    agent= Agent(eps,env.get_number_of_actions(),device)\n",
        "    if type_memory==0:\n",
        "        memory= ReplayMemory(prms_tra['memory_size'])\n",
        "        q= QDQ(device,env,agent,memory,prms_tra['features'])\n",
        "        q.training_replay_memory(prms_tra['batch_size'],prms_tra['gamma'],\n",
        "                            prms_tra['target_update'],prms_tra['lr'],prms_tra['num_episodes'])\n",
        "    elif type_memory==1:\n",
        "        memory= PrioritizedReplayMemory(prms_tra['memory_size'])\n",
        "        q= QDQ(device,env,agent,memory,prms_tra['features'])\n",
        "        q.training_priorized_memory(prms_tra['batch_size'],prms_tra['gamma'],\n",
        "                            prms_tra['target_update'],prms_tra['lr'],prms_tra['num_episodes'])\n",
        "    else:\n",
        "        print('Error: type_memory must be 0 or 1')\n",
        "        return 'Error' \n",
        "    return 0\n",
        "if __name__ == '__main__':\n",
        "    params_game = {\n",
        "                \"size\": 10,\n",
        "                \"start_position\": (9, 0),  # (9,0)\n",
        "                \"item_pickup\": (1, 1),  # (1,1)\n",
        "                \"item_dropoff\": (8, 8),  # (8,8)\n",
        "                \"zones_block\": [(4, 0), (4, 1), (4, 2), (4, 3), (2, 6), (2, 7), (2, 8), (2, 9), \n",
        "                                (4, 8), (5, 8), (6, 8), (7, 6), (8, 6), (9, 6)],\n",
        "                \"Path\": 'Episodes'\n",
        "    }\n",
        "    params_training = {\n",
        "                \"batch_size\": 1000,\n",
        "                \"features\": 1,\n",
        "                \"gamma\": 0.99,\n",
        "                \"eps_start\": 1,\n",
        "                \"eps_end\": 0.01,\n",
        "                \"eps_decay\": 0.001,\n",
        "                \"target_update\": 5000,\n",
        "                \"memory_size\": 100000,\n",
        "                \"lr\": 0.001,\n",
        "                \"num_episodes\": 10000\n",
        "    }\n",
        "    training_process(params_game,params_training,1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyORFVHJ/+QQBjAcOY4nd3jT",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
